# üéØ ResNet-18 ‚Äî CIFAR-100 Classification (From Scratch, 150 Epochs)

This repository contains a complete, end-to-end training pipeline for a **CIFAR-optimized ResNet-18** trained **from scratch** (no pretraining) on **CIFAR-100** for **150 epochs**.

The project demonstrates:

- A clean, minimal PyTorch training pipeline.
- Strong regularization and augmentation for small-image datasets.
- High-performance training on AWS EC2 GPUs.
- Full logging of **150 epochs** (train/test loss + accuracy).
- **Grad-CAM** interpretability visualizations.
- A deployed **HuggingFace Space** for live inference.

The implementation exceeds the assignment goal of *‚â•73%* top-1 accuracy and achieves:

> **‚≠ê Final Test Accuracy: 77.00% @ Epoch 150**  

> **üèÜ Best Test Accuracy: 77.05% @ Epoch 144**

---

## üìÅ Repository Structure

```text
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CIFAR-100_v3.1.ipynb             # Full training notebook
‚îú‚îÄ‚îÄ training_logs_v2.md              # All 150 epoch logs
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ best_resnet_cifar100.pth     # Best checkpoint (epoch 144)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ model.py                     # CIFAR-optimized ResNet-18
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                   # Dataloaders + augmentations
‚îÇ   ‚îú‚îÄ‚îÄ train.py                     # Script training pipeline
‚îÇ   ‚îî‚îÄ‚îÄ gradcam_utils.py             # Grad-CAM implementation
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ gradcam/
        ‚îú‚îÄ‚îÄ gradcam_sample_0.png
        ‚îú‚îÄ‚îÄ gradcam_sample_1.png
        ‚îî‚îÄ‚îÄ gradcam_sample_2.png

```
------

## üß† Model Architecture (CIFAR-Optimized ResNet-18)

High-level structure

This is a ResNet-18 style network adapted for 32√ó32 CIFAR images:
	‚Ä¢	No 7√ó7 conv or max-pooling at the start.
	‚Ä¢	Stem is a 3√ó3 conv with stride=1.
	‚Ä¢	Four residual stages, each with 2 BasicBlocks.
	‚Ä¢	Downsampling happens at the first block of layer2, layer3, and layer4 via stride=2.
	‚Ä¢	Final head: Global Average Pool (1√ó1) ‚Üí Dropout(p=0.3) ‚Üí FC(512 ‚Üí 100).

```text
Input (3√ó32√ó32)
  ‚Üì
Conv3√ó3 (3‚Üí64, stride 1) + BN + ReLU
  ‚Üì
Layer1: 2 √ó BasicBlock(64‚Üí64, stride 1)
  ‚Üì
Layer2: 2 √ó BasicBlock(64‚Üí128 then 128‚Üí128, first block stride 2)
  ‚Üì
Layer3: 2 √ó BasicBlock(128‚Üí256 then 256‚Üí256, first block stride 2)
  ‚Üì
Layer4: 2 √ó BasicBlock(256‚Üí512 then 512‚Üí512, first block stride 2)
  ‚Üì
AdaptiveAvgPool2d(1√ó1) ‚Üí Dropout(0.3) ‚Üí Linear(512‚Üí100)
```


Total params (from your code):
	‚Ä¢	**Total parameters**: 11,220,132
	‚Ä¢	**Trainable parameters**: 11,220,132
	‚Ä¢	**Frozen parameters**: 0

-----

## üîç Receptive Field & Dimensions ‚Äî Block-Level Table

Assumptions:
	‚Ä¢	Input image: **3** √ó **32** √ó **32**
	‚Ä¢	All convolutions: kernel=3, padding=1, unless explicitly noted.
	‚Ä¢	Receptive field (RF) is computed at the output of each block, starting with RF=1 at the input pixel.
	‚Ä¢	RF update rule:
		‚Ä¢	`jump_l = jump_(l-1) * stride_l`
		‚Ä¢	`RF_l   = RF_(l-1) + (kernel_l - 1) * jump_(l-1)`

Here‚Äôs the block-level summary:

| Stage      | Block    | In_C ‚Üí Out_C | Kernel / Stride / Pad | Input Dim (HxW) | Output Dim (HxW) | Params (block) | RF at block output |
| ---------- | -------- | ------------ | --------------------- | --------------- | ---------------- | -------------- | ------------------ |
| **Stem**   | conv1    | 3 ‚Üí 64       | 3√ó3 / 1 / 1           | 32√ó32           | 32√ó32            | 1,856          | 3                  |
| **Layer1** | Block1   | 64 ‚Üí 64      | (3√ó3,1,1)√ó2           | 32√ó32           | 32√ó32            | 73,984         | 7                  |
|            | Block2   | 64 ‚Üí 64      | (3√ó3,1,1)√ó2           | 32√ó32           | 32√ó32            | 73,984         | 11                 |
| **Layer2** | Block1   | 64 ‚Üí 128     | conv1: 3√ó3 /2 /1      | 32√ó32           | 16√ó16            | 230,144        | 17                 |
|            |          |              | conv2: 3√ó3 /1 /1      |                 |                  |                |                    |
|            |          |              | shortcut: 1√ó1 /2 /0   |                 |                  |                |                    |
|            | Block2   | 128 ‚Üí 128    | (3√ó3,1,1)√ó2           | 16√ó16           | 16√ó16            | 295,424        | 25                 |
| **Layer3** | Block1   | 128 ‚Üí 256    | conv1: 3√ó3 /2 /1      | 16√ó16           | 8√ó8              | 919,040        | 37                 |
|            |          |              | conv2: 3√ó3 /1 /1      |                 |                  |                |                    |
|            |          |              | shortcut: 1√ó1 /2 /0   |                 |                  |                |                    |
|            | Block2   | 256 ‚Üí 256    | (3√ó3,1,1)√ó2           | 8√ó8             | 8√ó8              | 1,180,672      | 53                 |
| **Layer4** | Block1   | 256 ‚Üí 512    | conv1: 3√ó3 /2 /1      | 8√ó8             | 4√ó4              | 3,673,088      | 77                 |
|            |          |              | conv2: 3√ó3 /1 /1      |                 |                  |                |                    |
|            |          |              | shortcut: 1√ó1 /2 /0   |                 |                  |                |                    |
|            | Block2   | 512 ‚Üí 512    | (3√ó3,1,1)√ó2           | 4√ó4             | 4√ó4              | 4,720,640      | **109**            |
| **Head**   | GAP + FC | 512 ‚Üí 100    | GAP 4√ó4 ‚Üí 1√ó1; FC     | 4√ó4 ‚Üí 1√ó1       | 1√ó1              | 51,300         | 109                |


-----

## üß™ Dataset & Augmentations

### Dataset: CIFAR-100
	‚Ä¢	50,000 train, 10,000 test
	‚Ä¢	100 classes, 32√ó32 RGB

### Normalization:
	‚Ä¢	Mean: **(0.5071, 0.4867, 0.4408)**
	‚Ä¢	Std:  **(0.2675, 0.2565, 0.2761)**

### Training augmentations:
	‚Ä¢	`RandomCrop(32, padding=4)`
	‚Ä¢	`RandomHorizontalFlip(p=0.5)`
	‚Ä¢	`RandomRotation(¬±15¬∞)`
	‚Ä¢	`ColorJitter (brightness, contrast, saturation, hue)`
	‚Ä¢	`RandomErasing(p=0.5)`
	‚Ä¢	`CutMix with Beta(Œ±=1.0) (applied with some probability)`

These are tuned for long training (150 epochs) so the model keeps seeing varied views and doesn‚Äôt just memorize.

-----

## ‚öôÔ∏è Training Configuration

**Hardware**
	‚Ä¢	AWS **EC2 g5.2xlarge
	‚Ä¢	NVIDIA A10G (24 GB)**
	‚Ä¢	Wall-clock training time: ~45‚Äì60 minutes for 150 epochs with AMP

**Optimizer & Loss**
	‚Ä¢	Optimizer: `SGD(lr=0.1, momentum=0.9, weight_decay=5e-4)`
	‚Ä¢	Scheduler: `OneCycleLR over 150 epochs`
	‚Ä¢	Loss: `CrossEntropyLoss(label_smoothing=0.1)`
	‚Ä¢	Precision: `torch.amp.autocast("cuda") + GradScaler("cuda")`

-----

## üìà Training Progress (`from training_logs_v2.md`)

A small snapshot (exact values from your logs):

| Epoch | Train Loss | Train Acc (%) | Test Loss | Test Acc (%) | Notes                          |
| ----: | ---------: | ------------: | --------: | -----------: | ------------------------------ |
|     1 |     4.3685 |          5.18 |    3.9975 |        10.94 | Initial convergence            |
|    25 |     2.6839 |         46.40 |    2.2202 |        54.12 | Strong feature learning        |
|    50 |     2.2324 |         59.39 |    1.8846 |        65.02 | Solid mid-training performance |
|    75 |     2.2250 |         60.85 |    1.8608 |        67.15 | Good generalization            |
|   100 |     1.9964 |         69.05 |    1.7489 |        69.70 | Near target (‚â•73%)             |
|   125 |     1.6978 |         78.22 |    1.6642 |        73.28 | Crosses assignment target      |
|   140 |     1.5568 |         82.37 |    1.5493 |        76.45 | Very strong performance        |
|   144 |     1.5960 |         81.17 |    1.5465 |    **77.05** | **Best test accuracy**         |
|   150 |     1.6258 |         82.02 |    1.5485 |    **77.00** | Final model                    |


Full logs (all 150 epochs) are in:
	‚Ä¢	[`training_logs_v2.md`](https://github.com/SXD390/ERA_CIFAR_100_ResNET_18/blob/main/training_logs_v2.md)

-----

## üî• Grad-CAM Visualizations

Grad-CAM is computed using the last conv in the last block:
```py
target_layer = model.layer4[1].conv2  # or layer4[-1].conv2
```
For an input image:`
	**1.**	Forward pass ‚Üí logits.
	**2.**	Pick predicted class (or any target class).
	**3.**	Backprop from that scalar logit.
	**4.**	Compute channel-wise weights via global average pooling of gradients.
	**5.**	Weighted sum of activations ‚Üí ReLU ‚Üí normalize ‚Üí upsample.
	**6.**	Overlay on the original image.

Example outputs:

<p float="left">
  <img src="assets/gradcam/class_000_apple.png" width="220" />
  <img src="assets/gradcam/class_006_bee.png" width="220" />
  <img src="assets/gradcam/class_012_bridge.png" width="220" />
    <img src="assets/gradcam/class_022_clock.png" width="220" />
	<img src="assets/gradcam/class_085_tank.png" width="220" />
	<img src="assets/gradcam/class_078_snake.png" width="220" />
	<img src="assets/gradcam/class_098_woman.png" width="220" />
	<img src="assets/gradcam/class_037_house.png" width="220" />
</p>


These show the network focusing on semantically meaningful regions of the object.

----

### ü§ó HuggingFace Space

A live demo is deployed on HuggingFace Spaces (Gradio):
	‚Ä¢	URL: `https://huggingface.co/spaces/<your-username>/<your-space-name>`

Features:
	‚Ä¢	Upload any image.
	‚Ä¢	Resizes and normalizes using CIFAR-100 stats.
	‚Ä¢	Runs it through `best_resnet_cifar100.pth.`
	‚Ä¢	Returns top-5 predictions with probabilities.

----

## üõ† Running Locally

Install dependencies
```py
pip3 install -r requirements.txt
```
### Option 1: Re-run training
```py
python src/train.py \
  --data-dir ./data \
  --batch-size 512 \
  --epochs 150 \
  --lr 0.1 \
  --weight-decay 5e-4 \
  --num-workers 4
```
### Option 2: Use the notebook
```bash
jupyter notebook CIFAR-100_v3.1.ipynb
```
Run all cells to:
	‚Ä¢	Download CIFAR-100.
	‚Ä¢	Build the model.
	‚Ä¢	Train (optional).
	‚Ä¢	Load the best checkpoint.
	‚Ä¢	Generate Grad-CAM visualizations.

----

‚úÖ Summary
	‚Ä¢	**Architecture**: CIFAR-optimized ResNet-18 with detailed RF and param analysis.
	‚Ä¢	**Training**: 150 epochs from scratch on CIFAR-100.
	‚Ä¢	**Hardware**: EC2 g5.2xlarge (A10G GPU).
	‚Ä¢	**Best Test Accuracy**: 77.05% @ epoch 144.
	‚Ä¢	**Final Test Accuracy**: 77.00% @ epoch 150.
	‚Ä¢	**Regularization**: CutMix, Random Erasing, Label Smoothing, Dropout.
	‚Ä¢	**Interpretability**: Grad-CAM integrated.
	‚Ä¢	**Deployment**: Live HuggingFace Space.
